# -*- coding: utf-8 -*-
"""GroupAssignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RcmdXAADALDK6Tz-wTqotMlHi8jAFzGv
"""

import numpy as np
import pandas as pd
from matplotlib.pyplot import subplots
import statsmodels.api as sm
!pip install ISLP
from ISLP import load_data
from ISLP.models import (ModelSpec as MS,
                         summarize)
from ISLP import confusion_table
from ISLP.models import contrast
from sklearn.discriminant_analysis import \
     (LinearDiscriminantAnalysis as LDA,
      QuadraticDiscriminantAnalysis as QDA)
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
!pip install seaborn
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

from google.colab import drive
drive.mount('/content/drive')

data_path = '/content/drive/MyDrive/BUS310_MinhTran/Data_BUS310/housing_price_full_sample.csv'  # Update with your actual data file name
df = pd.read_csv(data_path)

"""# 1. Data Organization"""

missing_values=df.isnull().sum()

missing_values=missing_values[missing_values>0]

print(missing_values)

"""1.1"""

# Impute missing data
# For numerical variables: use mean
numeric_cols=df.select_dtypes(include=['number']).columns
categorical_cols=df.select_dtypes(include=['object']).columns

num_imputer = SimpleImputer(strategy='mean')
df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])

cat_imputer = SimpleImputer(strategy='most_frequent')
df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])

"""1.2"""

categorical_vars = df.select_dtypes(include=['object', 'category'])

categorical_var_names=categorical_vars.columns.tolist()
print('Categorical Variables', categorical_var_names)

num_categorical_vars=len(categorical_var_names)
print('Number of Categorical Variables:', num_categorical_vars)

dummies=pd.get_dummies(df[categorical_var_names])
df=pd.concat([df, dummies], axis=1)
print(df.head())

"""1.3"""

categorical_var_names=['MSSubClass', 'MoSold', 'YearBuilt']

df_organized=pd.get_dummies(df, columns=categorical_var_names, drop_first=False)

unique_years_count=df['YrSold'].nunique()
print('Number of Unique values in YrSold:', unique_years_count)

"""###2 Explanatory Data Analysis"""

sales_price_stats=df['SalePrice'].describe().round(2)
print(sales_price_stats)

plt.figure(figsize=(10, 6))
sns.boxplot(df['SalePrice'])
plt.title('Distribution of SalePrice')
plt.ylabel('Sales Price')
plt.grid()
plt.show()

plt.figure(figsize=(10,6))
count,bins,ignored = plt.hist(df['SalePrice'], 30, density=True,color='skyblue',edgecolor='black',alpha=0.6)

kde = sns.kdeplot(df['SalePrice'],color = 'red',lw=2)
plt.title("Histogram of Sale Price with KDE")
plt.xlabel("Sale Price")
plt.ylabel("Density")
plt.grid(axis='y')
plt.show()

numeric_vars=df.select_dtypes(include=['number'])
print(numeric_vars.head())

cor_num_var=numeric_vars.corr()
print(cor_num_var)

cor_sorted=cor_num_var['SalePrice'].sort_values(ascending=False)

top_10_cor_high=cor_sorted.head(10)
cor_num_var_high=cor_num_var.loc[top_10_cor_high.index, top_10_cor_high.index]

plt.figure(figsize=(10, 8))
sns.heatmap(cor_num_var_high, annot=True, cmap='coolwarm', fmt=".2f", linewidth=.5, cbar=True, square=True)
plt.title('Top 10 Highly Correlated Variables with SalesPrice')
plt.show()

"""Box Plot for Overall Quality"""

plt.figure(figsize=(10, 5))
sns.boxplot(x='OverallQual', y='SalePrice', data=df)
plt.title('SalePrice by Overall Quality')
plt.xlabel('Overall Quality')
plt.ylabel('SalePrice')
plt.show()

"""Scatter Plot for GrLivArea"""

plt.figure(figsize=(10, 5))
sns.regplot(x='GrLivArea', y='SalePrice', data=df, ci=None)
plt.title('SalePrice vs GrLivArea')
plt.xlabel('Above Grade Living Area (sq ft)')
plt.ylabel('SalePrice')
plt.show()

"""Bar Charts for Year Sold and Month Sold:"""

# Average SalePrice by Year Sold
year_avg_price = df.groupby('YrSold')['SalePrice'].mean().reset_index()
year_count = df['YrSold'].value_counts().reset_index()

plt.figure(figsize=(15, 6))
sns.barplot(x='YrSold', y='SalePrice', data=year_avg_price)
plt.title('Average SalePrice by Year Sold')
plt.ylabel('Average SalePrice')
plt.show()

plt.figure(figsize=(15, 6))
sns.barplot(x='YrSold', y='YrSold', data=year_count)
plt.title('Number of Sales by Year Sold')
plt.xlabel('YrSold')
plt.ylabel('Number of Sales')
plt.show()

# Average SalePrice by Month Sold
month_avg_price = df.groupby('MoSold')['SalePrice'].mean().reset_index()
month_count = df['MoSold'].value_counts().reset_index()

plt.figure(figsize=(15, 6))
sns.barplot(x='MoSold', y='SalePrice', data=month_avg_price)
plt.title('Average SalePrice by Month Sold')
plt.ylabel('Average SalePrice')
plt.show()

plt.figure(figsize=(15, 6))
sns.barplot(x='MoSold', y='MoSold', data=month_count)
plt.title('Number of Sales by Month Sold')
plt.xlabel('Month Sold')
plt.ylabel('Number of Sales')
plt.show()

"""Bar Charts for Neighborhood and MSSubClass"""

# Average SalePrice and Count for Neighborhood
neighborhood_avg_price = df.groupby('Neighborhood')['SalePrice'].median().reset_index()
neighborhood_count = df['Neighborhood'].value_counts().reset_index()

plt.figure(figsize=(15, 6))
sns.barplot(x='Neighborhood', y='SalePrice', data=neighborhood_avg_price)
plt.title('Median SalePrice by Neighborhood')
plt.xticks(rotation=90)
plt.ylabel('Median SalePrice')
plt.show()

plt.figure(figsize=(15, 6))
sns.barplot(x='Neighborhood', y='Neighborhood', data=neighborhood_count)
plt.title('Number of Sales by Neighborhood')
plt.xticks(rotation=90)
plt.xlabel('Neighborhood')
plt.ylabel('Number of Sales')
plt.show()

# Average SalePrice and Count for MSSubClass
msclass_avg_price = df.groupby('MSSubClass')['SalePrice'].median().reset_index()
msclass_count = df['MSSubClass'].value_counts().reset_index()

plt.figure(figsize=(15, 6))
sns.barplot(x='MSSubClass', y='SalePrice', data=msclass_avg_price)
plt.title('Median SalePrice by MSSubClass')
plt.ylabel('Median SalePrice')
plt.show()

plt.figure(figsize=(15, 6))
sns.barplot(x='MSSubClass', y='MSSubClass', data=msclass_count)
plt.title('Number of Sales by MSSubClass')
plt.xlabel('MSSubClass')
plt.ylabel('Number of Sales')
plt.show()

"""###3.1 Model Selection"""

cor_sorted=cor_num_var['SalePrice'].sort_values(ascending=False)

top_10_cor_high=cor_sorted.head(10)

top_10_cor_variable_names=top_10_cor_high.index.tolist()
df_high_corr=df[top_10_cor_variable_names]

print(df_high_corr)

"""Check correlation between predictors"""

corr_predictors=df_high_corr.drop(columns=['SalePrice']).corr()
print(corr_predictors)

"""###3.1.2"""

num_predictors=df_high_corr.columns.drop(['SalePrice'])
X=MS(num_predictors).fit_transform(df_high_corr)
y=df_high_corr['SalePrice']

model=sm.OLS(y,X)
results=model.fit()
print(results.summary())

X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)

lr_model_high_corr=LinearRegression()
lr_model_high_corr.fit(X_train, y_train)

y_pred=lr_model_high_corr.predict(X_test)

predictions_df_high_corr_model1=pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(predictions_df_high_corr_model1)

mse_model10 = mean_squared_error(y_test, y_pred)
r2_model10 = r2_score(y_test, y_pred)
print(f'MSE Model 10: {mse_model10}')
print(f'R2 Model 10: {r2_model10}')

"""High Corr Model"""

num_predictors2=df_high_corr.columns.drop(['SalePrice', 'FullBath', '1stFlrSF','GarageCars'])
X=MS(num_predictors2).fit_transform(df_high_corr)
y=df_high_corr['SalePrice']
high_corr_model=sm.OLS(y,X)
results2=high_corr_model.fit()
print(results2.summary())

"""Checking Model Predictor

**Printed: MSE and R squared value of model 1**
"""

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)

lr_model_high_corr=LinearRegression()
lr_model_high_corr.fit(X_train, y_train)

y_pred=lr_model_high_corr.predict(X_test)

predictions_df_high_corr_model1=pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(predictions_df_high_corr_model1)

mse_model1 = mean_squared_error(y_test, y_pred)
r2_model1 = r2_score(y_test, y_pred)
print(f'MSE Model 1: {mse_model1}')
print(f'R2 Model 1: {r2_model1}')

plt.scatter(y_test, y_pred)
plt.xlabel('Actual SalePrice')
plt.ylabel('Predicted SalePrice')
plt.title('Actual vs Predicted SalePrice Model 1')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')

plt.show()

"""### All variable model"""

numerical_and_boolean_df=df.select_dtypes(include=['number', 'bool'])
allvar=numerical_and_boolean_df.columns.drop(['SalePrice'])
X=MS(allvar).fit_transform(numerical_and_boolean_df)
y=df['SalePrice']

model6=sm.OLS(y,X)
results6=model6.fit()
print(results6.summary())

"""Drop insignificant predictors"""

significant_vars=results6.pvalues[results6.pvalues<0.05].index.tolist()

significant_X=X[significant_vars]
print(significant_X)

"""Models with all significant predictors"""

allvars=significant_X.columns
X=MS(allvars).fit_transform(significant_X)
y=df['SalePrice']

model=sm.OLS(y,X)
results=model7.fit()
print(results.summary())

"""3.2 Model Prediction

**Printed: MSE and R squared of model 2**
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X_train, X_test, y_train, y_test=train_test_split(significant_X, y, test_size=0.2, random_state=42)

#Fit the Linear Regression model
lr_model=LinearRegression()
lr_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred=lr_model.predict(X_test)

#Display the predictions
predictions_df=pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(predictions_df)

mse_model2 = mean_squared_error(y_test, lr_model.predict(X_test))
r2_model2 = r2_score(y_test, lr_model.predict(X_test))
print(f'MSE Model 2: {mse_model2}')
print(f'R2 Model 2: {r2_model2}')

model9=sm.OLS(y,X)
results9=model7.fit()
print(results7.summary())

import matplotlib.pyplot as plt

plt.scatter(y_test, y_pred)
plt.xlabel('Actual SalePrice')
plt.ylabel('Predicted SalePrice')
plt.title('Actual vs Predicted SalePrice Model 2')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.show()

"""***Compare MSE and R squared of Model 1 and Model 2***"""

plt.bar(['Model 1', 'Model 2'], [mse_model1, mse_model2])
print(mse_model1)
print(mse_model2)
plt.xlabel('Model')
plt.ylabel('MSE')
plt.title('Comparison of MSE')
plt.show()

performance_table = pd.DataFrame({
    'Model': ['Model 1', 'Model 2'],
    'MSE': [mse_model1, mse_model2],
    'R2': [r2_model1, r2_model2]
})
print(performance_table)

import seaborn as sns

# Select top 5 correlated features with SalePrice for pair plot
top_5_cor_vars = cor_num_var['SalePrice'].sort_values(ascending=False).index[1:6]

# Create a pair plot for the top 5 correlated features with SalePrice
sns.pairplot(df[top_5_cor_vars], diag_kind='kde')
plt.suptitle('Pair Plot of Top 5 Features Correlated with SalePrice', y=1.02)
plt.show()

"""#Three Extra Charts to help our Project"""

# Assuming the significant predictors were already selected (using OLS or another method)
X_train_significant, X_test_significant, y_train, y_test = train_test_split(significant_X, y, test_size=0.2, random_state=42)

# Fit a linear regression model
linear_model = LinearRegression()
linear_model.fit(X_train_significant, y_train)

# Create a DataFrame to store feature importances
feature_importances = pd.DataFrame({'Feature': significant_X.columns,
                                    'Importance': linear_model.coef_})

# Sort the features by their absolute importance values
feature_importances['AbsImportance'] = feature_importances['Importance'].abs()
feature_importances = feature_importances.sort_values(by='AbsImportance', ascending=False)

# Plotting feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='coolwarm')
plt.title('Feature Importance for Linear Regression Model')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature Name')
plt.show()

"""***Feature Importance Plot for a Linear Model***

This graph visualizes the coefficients of a linear model, showing which features are most influential in predicting SalePrice.

***Key Insights from the Graph:***

High Positive Impact Variables: Variables with large positive coefficients are critical drivers for increasing SalePrice. In many housing datasets, these are often variables like GrLivArea, OverallQual, or GarageArea.
High Negative Impact Variables: Variables with large negative coefficients indicate factors that decrease the housing price, such as HouseAge, poor condition, or certain features that detract from the property value.
Smaller Impact Variables: If a variable has a very small coefficient (close to zero), it means that changes in this variable have a minimal impact on predicting SalePrice.

***Real-World Interpretations:***

A large positive coefficient for GrLivArea suggests that homes with more above-grade living space command a higher price.
A negative coefficient for a feature like OverallCond might indicate that homes in poorer condition significantly reduce the SalePrice, even if they have other favorable attributes.

#Above Grade Living Area vs Sales Price
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 5))
sns.regplot(x='GrLivArea', y='SalePrice', data=df, ci=None, line_kws={'color': 'red'})
plt.title('SalePrice vs GrLivArea')
plt.xlabel('Above Grade Living Area (sq ft)')
plt.ylabel('SalePrice')
plt.show()

"""#Year House was Built vs Sales Price"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create a scatter plot with a trend line
plt.figure(figsize=(10, 6))
sns.regplot(x='YearBuilt', y='SalePrice', data=df, color='blue', line_kws={'color': 'red'})

# Customize the plot
plt.title('Correlation between Sale Price and Year Built')
plt.xlabel('Year Built')
plt.ylabel('Sale Price')
plt.grid(False)

# Show the plot
plt.show()

X_train2, X_test2, y_train2, y_test2 = train_test_split(significant_X, y, test_size=0.2, random_state=42)
lr_model2 = LinearRegression()
lr_model2.fit(X_train2, y_train2)
y_pred2 = lr_model2.predict(X_test2)
predictions_df2 = pd.DataFrame({'Actual': y_test2, 'Predicted': y_pred2})
print(predictions_df2)

plt.scatter(y_test2, y_pred2)
plt.xlabel('Actual SalePrice')
plt.ylabel('Predicted SalePrice')
plt.title('Actual vs Predicted SalePrice Model 2')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.show()